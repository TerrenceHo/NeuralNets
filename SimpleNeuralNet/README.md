# TODO List and Features to Implement

- Change db from scalar to vector of (layer size, 1)
- Activation Functions:
    - Softmax and derivative
    - Tanh and derivative
    - ~~make passing in activation functions functional~~
    - Pass activation functions into the cache to make it easily retrievable
- Gradient Descent:
    - Momentum
    - Minibatch
- RMSProp
- Adam optimizer
- Learning Rate Decay
- Gradient Checks
- Dropout layers
- Batch-Normalization
- Preprocessing Functions
