# TODO List and Features to Implement

- ~~Change db from scalar to vector of (layer size, 1)~~
- Activation Functions:
    - Softmax and derivative
    - Tanh and derivative
    - ~~make passing in activation functions functional~~
    - ~~Pass activation functions into the cache to make it easily retrievable~~
- Gradient Descent:
    - Momentum
    - ~~Minibatch~~
- RMSProp
- Adam optimizer
- Learning Rate Decay
- Gradient Checks
- Dropout layers
    -Bug concerning numerical stability
- Batch-Normalization
- Preprocessing Functions
