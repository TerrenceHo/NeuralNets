# TODO List and Features to Implement

- Change db from scalar to vector of (layer size, 1)
- Activation Functions:
    - Softmax and derivative
    - Tanh and derivative
    - Make passing in activation functions functional
- Gradient Descent:
    - Momentum
    - Minibatch
- RMSProp
- Adam optimizer
- Learning Rate Decay
- Gradient Checks
- Dropout layers

